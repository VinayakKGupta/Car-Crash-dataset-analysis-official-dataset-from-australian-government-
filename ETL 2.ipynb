{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951418a9",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ee93a2-2264-4a53-b8c6-ba0abf5bd19e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from shapely import MultiPolygon\n",
    "from shapely.wkt import dumps as wkt_dumps # Use Shapely directly for WKT\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Configure logging to display messages in the notebook output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", force=True\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b1c7a-ee7b-479d-a73b-264227e4bc43",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e7917b-e787-4907-ad92-a7532ba90c7e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fatal_crashes_df = pd.read_excel(\n",
    "    \"data/Project/bitre_fatal_crashes_dec2024.xlsx\",\n",
    "    sheet_name=\"BITRE_Fatal_Crash\",\n",
    "    skiprows=4,\n",
    "    na_values=[\"\", \"-9\"],\n",
    ")\n",
    "# Wrong header name fix\n",
    "fatal_crashes_df = fatal_crashes_df.rename(\n",
    "    columns={\"Bus \\nInvolvement\": \"Bus Involvement\", \"Time of Day\": \"Time of day\"},\n",
    ")\n",
    "fatalities_df = pd.read_excel(\n",
    "    \"data/Project/bitre_fatalities_dec2024.xlsx\",\n",
    "    sheet_name=\"BITRE_Fatality\",\n",
    "    skiprows=4,\n",
    "    na_values=[\"\", \"-9\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417288fc-3656-4b65-9c07-8681a4dcdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_LGA_df = pd.read_excel(\n",
    "    \"data/Project/Population estimates by LGA, Significant Urban Area, Remoteness Area, Commonwealth Electoral Division and State Electoral Division, 2001 to 2023.xlsx\",\n",
    "    sheet_name=\"Table 1\",\n",
    "    header=None,\n",
    "    skiprows=7,\n",
    "    skipfooter=2,\n",
    "    usecols=\"B:Y\",\n",
    ")\n",
    "pop_remoteness_df = pd.read_excel(\n",
    "    \"data/Project/Population estimates by LGA, Significant Urban Area, Remoteness Area, Commonwealth Electoral Division and State Electoral Division, 2001 to 2023.xlsx\",\n",
    "    sheet_name=\"Table 3\",\n",
    "    header=None,\n",
    "    skiprows=7,\n",
    "    skipfooter=7,\n",
    "    usecols=\"A:Y\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c653cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dwellings_df = pd.read_csv(\n",
    "    \"data/Project/LGA (count of dwellings).csv\",\n",
    "    skiprows=11,        # Skip the metadata header lines\n",
    "    skipfooter=9,       # Skip the metadata footer lines and Total row\n",
    "    usecols=range(2),   # Only keep the first two columns\n",
    "    engine=\"python\",    # Use Python engine to handle the footer\n",
    "    header=None,        # No header in the CSV\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf37f26",
   "metadata": {},
   "source": [
    "### Deal with unknown values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3548037a-c1d7-49c7-9d6c-83af9428b2f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deal with int value\n",
    "fatal_crashes_df.replace(-9, pd.NA, inplace=True)\n",
    "fatalities_df.replace(-9, pd.NA, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bafc71-6d20-41e5-bc14-310d6f831bf9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Merge Crash and Fatality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d37d82-3298-4157-9f4e-17181d790269",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df = pd.merge(\n",
    "    fatalities_df, fatal_crashes_df, on=\"Crash ID\", suffixes=(\"_victim\", \"_crash\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679529da-87a2-49b8-83cd-57e4d7180b70",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e763c6e1-64b2-4ff1-9d5a-1ca65c164c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crash IDs to be deleted (length ≠ 8):\n",
      "201850049\n",
      "201850039\n",
      "201850059\n",
      "201750029\n",
      "201750019\n",
      "201650079\n",
      "201650019\n",
      "201550039\n",
      "201550099\n",
      "201550059\n",
      "\n",
      "Total rows to be deleted: 10\n",
      "\n",
      "Remaining rows: 56864\n",
      "Crash ID length distribution in remaining data:\n",
      "Crash ID\n",
      "8    56864\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify rows to be deleted (where Crash ID length ≠ 8)\n",
    "rows_to_delete = fact_df[fact_df[\"Crash ID\"].astype(str).str.len() != 8]\n",
    "\n",
    "# Print only the Crash IDs that will be deleted\n",
    "print(\"Crash IDs to be deleted (length ≠ 8):\")\n",
    "print(rows_to_delete[\"Crash ID\"].to_string(index=False))\n",
    "\n",
    "print(f\"\\nTotal rows to be deleted: {len(rows_to_delete)}\")\n",
    "\n",
    "# Execute deletion (keep only rows where Crash ID length equals 8)\n",
    "fact_df = fact_df[fact_df[\"Crash ID\"].astype(str).str.len() == 8]\n",
    "\n",
    "# Verification\n",
    "print(f\"\\nRemaining rows: {len(fact_df)}\")\n",
    "print(\"Crash ID length distribution in remaining data:\")\n",
    "print(fact_df[\"Crash ID\"].astype(str).str.len().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3928d9b9-06d9-4dc0-aa95-88934dc4034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crash IDs with empty Time field to be deleted:\n",
      "20247030\n",
      "20247040\n",
      "20247033\n",
      "20127002\n",
      "20087036\n",
      "20087041\n",
      "20077047\n",
      "20077047\n",
      "20057036\n",
      "20057045\n",
      "20057045\n",
      "20052184\n",
      "20057028\n",
      "20042247\n",
      "20036028\n",
      "20022015\n",
      "20012201\n",
      "20012064\n",
      "20012134\n",
      "20012165\n",
      "20002042\n",
      "19982058\n",
      "19988013\n",
      "19988020\n",
      "19986046\n",
      "19962220\n",
      "19958014\n",
      "19958004\n",
      "19958001\n",
      "19958001\n",
      "19952079\n",
      "19952356\n",
      "19958003\n",
      "19946050\n",
      "19935187\n",
      "19935187\n",
      "19935161\n",
      "19935123\n",
      "19935191\n",
      "19912147\n",
      "19915105\n",
      "19902391\n",
      "19895029\n",
      "\n",
      "Total rows to be deleted: 43\n",
      "\n",
      "Remaining rows: 56821\n",
      "Number of null Time values after deletion: 0\n"
     ]
    }
   ],
   "source": [
    "# Identify rows with empty Time field\n",
    "empty_time_rows = fact_df[fact_df[\"Time_victim\"].isna()]\n",
    "\n",
    "# Print only the Crash IDs of rows with empty Time (assuming Crash ID exists)\n",
    "if not empty_time_rows.empty:\n",
    "    print(\"Crash IDs with empty Time field to be deleted:\")\n",
    "    print(empty_time_rows[\"Crash ID\"].to_string(index=False))\n",
    "else:\n",
    "    print(\"No rows with empty Time field found\")\n",
    "\n",
    "print(f\"\\nTotal rows to be deleted: {len(empty_time_rows)}\")\n",
    "\n",
    "# Execute deletion (keep only rows with non-empty Time)\n",
    "fact_df = fact_df[fact_df[\"Time_victim\"].notna()]\n",
    "\n",
    "# Verification\n",
    "print(f\"\\nRemaining rows: {len(fact_df)}\")\n",
    "print(\n",
    "    f\"Number of null Time values after deletion: {fact_df['Time_victim'].isna().sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e467114-990b-4948-a3b2-ce3419263e0b",
   "metadata": {},
   "source": [
    "### Special Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70f55016-12a6-4741-845f-22cd705d974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set missing age_group\n",
    "fact_df.loc[fact_df[\"Crash ID\"] == 20225110, \"Age Group\"] = \"17_to_25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa1c3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix missing Time of day\n",
    "fact_df.loc[fact_df[\"Crash ID\"] == 20225066, \"Time of day_victim\"] = \"Day\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff1b33-1f19-40cd-b9c6-3a11e0f5900b",
   "metadata": {},
   "source": [
    "## Create Dimension Tables (Use fatalities_df as main table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a54913",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mapping = {\n",
    "    \"New South Wales\": \"NSW\",\n",
    "    \"Victoria\": \"Vic\",\n",
    "    \"Queensland\": \"Qld\", # Make sure abbreviation matches your data\n",
    "    \"Western Australia\": \"WA\",\n",
    "    \"South Australia\": \"SA\",\n",
    "    \"Tasmania\": \"Tas\",\n",
    "    \"Northern Territory\": \"NT\",\n",
    "    \"Australian Capital Territory\": \"ACT\",\n",
    "    # Add others if necessary (e.g., Other Territories)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb304b-c608-4b8b-8376-7552b7a2bb54",
   "metadata": {},
   "source": [
    "### dim_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4870c7e3-d8a6-467d-9470-b2bc91d81256",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date_df = fact_df[\n",
    "    [\n",
    "        \"Year_victim\",\n",
    "        \"Month_victim\",\n",
    "        \"Christmas Period_victim\",\n",
    "        \"Easter Period_victim\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "dim_date_df.rename(\n",
    "    columns={\n",
    "        \"Year_victim\": \"year\",\n",
    "        \"Month_victim\": \"month\",\n",
    "        \"Christmas Period_victim\": \"christmas_period\",\n",
    "        \"Easter Period_victim\": \"easter_period\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Derive additional columns\n",
    "month_map = {\n",
    "    1: \"January\",\n",
    "    2: \"February\",\n",
    "    3: \"March\",\n",
    "    4: \"April\",\n",
    "    5: \"May\",\n",
    "    6: \"June\",\n",
    "    7: \"July\",\n",
    "    8: \"August\",\n",
    "    9: \"September\",\n",
    "    10: \"October\",\n",
    "    11: \"November\",\n",
    "    12: \"December\",\n",
    "}\n",
    "dim_date_df[\"month_name\"] = dim_date_df[\"month\"].map(month_map)\n",
    "dim_date_df[\"quarter\"] = ((dim_date_df[\"month\"] - 1) // 3) + 1\n",
    "dim_date_df[\"month_year\"] = (\n",
    "    dim_date_df[\"year\"].astype(str) + \"-\" + dim_date_df[\"month\"].astype(str).str.zfill(2)\n",
    ")\n",
    "\n",
    "# Select columns in the final table order and drop duplicates\n",
    "dim_date_df = (\n",
    "    dim_date_df[\n",
    "        [\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"month_name\",\n",
    "            \"quarter\",\n",
    "            \"month_year\",\n",
    "            \"christmas_period\",\n",
    "            \"easter_period\",\n",
    "        ]\n",
    "    ]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80613dd2-f334-4ff6-b011-d2ecbb2c8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date_df.to_csv(\"output/dim_date.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aabfaf-507b-480f-ae6a-d4fc6cffe9b1",
   "metadata": {},
   "source": [
    "### dim_time_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b359571-6df4-4f96-a3fe-1e7fdc73530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_timeofday_df = fact_df[[\"Time_victim\", \"Time of day_victim\"]].copy()\n",
    "\n",
    "dim_timeofday_df.rename(\n",
    "    columns={\n",
    "        \"Time_victim\": \"time\",\n",
    "        \"Time of day_victim\": \"time_of_day_category\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Convert to time objects to handle potential variations and extract components\n",
    "# Handle potential errors during conversion if format is inconsistent\n",
    "try:\n",
    "    dim_timeofday_df[\"time\"] = pd.to_datetime(\n",
    "        dim_timeofday_df[\"time\"], format=\"%H:%M:%S\", errors=\"coerce\"\n",
    "    ).dt.time\n",
    "except ValueError:\n",
    "    print(\n",
    "        \"Warning: Could not parse all time values. Check format consistency (HH:MM:SS). Using NaT for unparseable times.\"\n",
    "    )\n",
    "    dim_timeofday_df[\"time\"] = pd.to_datetime(\n",
    "        dim_timeofday_df[\"time\"], errors=\"raise\"  # Raise error if format is inconsistent\n",
    "    ).dt.time  # Try without format\n",
    "\n",
    "dim_timeofday_df[\"hour_24\"] = pd.to_datetime(\n",
    "    dim_timeofday_df[\"time\"].astype(str), format=\"%H:%M:%S\"\n",
    ").dt.hour.astype('Int64')\n",
    "\n",
    "\n",
    "# Select final columns and drop duplicates\n",
    "dim_timeofday_df = (\n",
    "    dim_timeofday_df[[\"time_of_day_category\", \"hour_24\", \"time\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4702e7f7-bbfd-4ffa-867e-183f2e13ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_timeofday_df.to_csv(\"output/dim_timeofday.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb9cb7-49f6-4d91-b8a0-7d571c37ebf1",
   "metadata": {},
   "source": [
    "### dim_road_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "962836e7-3a10-4d21-96e9-27093cc038df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_roaduser_df = fact_df[[\"Road User\", \"Gender\", \"Age\", \"Age Group\"]].copy()\n",
    "dim_roaduser_df.rename(\n",
    "    columns={\n",
    "        \"Road User\": \"road_user_type\",\n",
    "        \"Gender\": \"gender\",\n",
    "        \"Age\": \"age\",\n",
    "        \"Age Group\": \"age_group\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "dim_roaduser_df[\"gender\"] = dim_roaduser_df[\"gender\"].fillna(\"Unknown\")\n",
    "dim_roaduser_df[\"age_group\"] = dim_roaduser_df[\"age_group\"].fillna(\"Unknown\")\n",
    "\n",
    "dim_roaduser_df = (\n",
    "    dim_roaduser_df[[\"road_user_type\", \"gender\", \"age\", \"age_group\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bade0921-c30a-4a1f-8e3a-a944874d816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_roaduser_df.to_csv(\"output/dim_roaduser.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105c955-73d0-4a14-9dd2-ba49b9dd762b",
   "metadata": {},
   "source": [
    "### dim_crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dbe4f46-10e5-48e2-9916-a3c9e4d38eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_crash_df = fact_df[\n",
    "    [\n",
    "        \"Crash ID\",\n",
    "        \"Crash Type_victim\",\n",
    "        \"Bus Involvement_victim\",\n",
    "        \"Heavy Rigid Truck Involvement_victim\",\n",
    "        \"Articulated Truck Involvement_victim\",\n",
    "        \"Speed Limit_victim\",\n",
    "        \"National Road Type_victim\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "dim_crash_df.rename(\n",
    "    columns={\n",
    "        \"Crash ID\": \"crash_id\",\n",
    "        \"Crash Type_victim\": \"crash_type\",\n",
    "        \"Bus Involvement_victim\": \"bus_involvement\",\n",
    "        \"Heavy Rigid Truck Involvement_victim\": \"heavy_rigid_truck_involvement\",\n",
    "        \"Articulated Truck Involvement_victim\": \"articulated_truck_involvement\",\n",
    "        \"Speed Limit_victim\": \"speed_limit\",\n",
    "        \"National Road Type_victim\": \"national_road_type\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "columns_to_fill = [\n",
    "    \"crash_type\",\n",
    "    \"bus_involvement\",\n",
    "    \"heavy_rigid_truck_involvement\",\n",
    "    \"articulated_truck_involvement\",\n",
    "    \"speed_limit\",\n",
    "    \"national_road_type\",\n",
    "]\n",
    "dim_crash_df[columns_to_fill] = dim_crash_df[columns_to_fill].fillna(\"Unknown\")\n",
    "# Keep only the first occurrence for each Crash_ID to define the dimension attributes\n",
    "dim_crash_df = dim_crash_df.drop_duplicates(subset=[\"crash_id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7748d00-7842-4833-873b-68bc7a8260b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_crash_df = dim_crash_df[\n",
    "    [\n",
    "        \"crash_id\",\n",
    "        \"crash_type\",\n",
    "        \"bus_involvement\",\n",
    "        \"heavy_rigid_truck_involvement\",\n",
    "        \"articulated_truck_involvement\",\n",
    "        \"speed_limit\",\n",
    "        \"national_road_type\",\n",
    "    ]\n",
    "]\n",
    "dim_crash_df.to_csv(\"output/dim_crash.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5e28e-a47d-4078-af87-0c6139a3c868",
   "metadata": {},
   "source": [
    "### dim_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b248f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOJSON_PATH = \"data/Project/Geojson files/\"  # Base path to GeoJSONs\n",
    "OUTPUT_CSV_FILE = \"output/dim_location.csv\"  # Output CSV file\n",
    "LGA_TARGET_SRID = 4283\n",
    "SA4_STATE_TARGET_SRID = 7844"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680e6db",
   "metadata": {},
   "source": [
    "#### Step 1: Extract Unique Location Attributes\n",
    "\n",
    "This cell extracts the necessary attribute columns from fact_df, renames them for consistency with the target table, and finds the unique combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8dda4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 22:46:36,383 - INFO - Extracting unique location attributes from fact_df...\n",
      "2025-04-09 22:46:36,427 - INFO - Found 802 unique location attribute combinations.\n",
      "2025-04-09 22:46:36,427 - INFO - Displaying unique location attributes (head):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>sa4_name_2021</th>\n",
       "      <th>lga_name_2021</th>\n",
       "      <th>remoteness_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW</td>\n",
       "      <td>Riverina</td>\n",
       "      <td>Wagga Wagga</td>\n",
       "      <td>Inner Regional Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW</td>\n",
       "      <td>Sydney - Baulkham Hills and Hawkesbury</td>\n",
       "      <td>Hawkesbury</td>\n",
       "      <td>Inner Regional Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tas</td>\n",
       "      <td>Launceston and North East</td>\n",
       "      <td>Northern Midlands</td>\n",
       "      <td>Inner Regional Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW</td>\n",
       "      <td>New England and North West</td>\n",
       "      <td>Armidale Regional</td>\n",
       "      <td>Outer Regional Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vic</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state                           sa4_name_2021      lga_name_2021  \\\n",
       "0   NSW                                Riverina        Wagga Wagga   \n",
       "1   NSW  Sydney - Baulkham Hills and Hawkesbury         Hawkesbury   \n",
       "2   Tas               Launceston and North East  Northern Midlands   \n",
       "3   NSW              New England and North West  Armidale Regional   \n",
       "4   Vic                                 Unknown            Unknown   \n",
       "\n",
       "            remoteness_area  \n",
       "0  Inner Regional Australia  \n",
       "1  Inner Regional Australia  \n",
       "2  Inner Regional Australia  \n",
       "3  Outer Regional Australia  \n",
       "4                   Unknown  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logging.info(\"Extracting unique location attributes from fact_df...\")\n",
    "location_attributes_df = fact_df[\n",
    "    [\n",
    "        \"State_victim\",\n",
    "        \"SA4 Name 2021_victim\",\n",
    "        \"National LGA Name 2021_victim\",\n",
    "        \"National Remoteness Areas_victim\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# Rename columns\n",
    "location_attributes_df.rename(\n",
    "    columns={\n",
    "        \"State_victim\": \"state\",\n",
    "        \"SA4 Name 2021_victim\": \"sa4_name_2021\",\n",
    "        \"National LGA Name 2021_victim\": \"lga_name_2021\",\n",
    "        \"National Remoteness Areas_victim\": \"remoteness_area\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "location_attributes_df.fillna(\"Unknown\", inplace=True)\n",
    "# Drop exact duplicates\n",
    "unique_locations_df = location_attributes_df.drop_duplicates().reset_index(drop=True)\n",
    "logging.info(\n",
    "    f\"Found {len(unique_locations_df)} unique location attribute combinations.\"\n",
    ")\n",
    "\n",
    "logging.info(\"Displaying unique location attributes (head):\")\n",
    "display(unique_locations_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a10718",
   "metadata": {},
   "source": [
    "#### Step 2: Load Geometries\n",
    "\n",
    "Define a helper function to load GeoJSON files for each administrative level and then call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e55a3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geometries(geojson_filename, name_property, expected_level, crs=f\"EPSG:7844\", base_path=GEOJSON_PATH):\n",
    "    \"\"\"\n",
    "    Loads geometries from a SINGLE specified GeoJSON file.\n",
    "\n",
    "    Args:\n",
    "        geojson_filename (str): The specific filename (e.g., 'sa4_boundaries_2021.geojson').\n",
    "        name_property (str): The property name in the GeoJSON containing the location name\n",
    "        (e.g., 'LGA_NAME21', 'SA4_NAME21', 'STATE_NAME').\n",
    "        expected_level (str): The level name (e.g., 'LGA', 'SA4', 'State') for logging.\n",
    "        crs (str): The target Coordinate Reference System.\n",
    "        base_path (str): Optional base directory where the GeoJSON file is located.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: GeoDataFrame for the file, or None if processing fails.\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(base_path, geojson_filename)\n",
    "    logging.info(f\"Attempting to load {expected_level} geometries from: {full_path} using name property: {name_property}...\")\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        logging.error(f\"File not found: {full_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        gdf = gpd.read_file(full_path)\n",
    "\n",
    "        # --- Basic Validation ---\n",
    "        if 'geometry' not in gdf.columns:\n",
    "            logging.error(f\"File {geojson_filename}: No 'geometry' column found.\")\n",
    "            return None\n",
    "        if name_property not in gdf.columns:\n",
    "            logging.error(f\"File {geojson_filename}: Required property '{name_property}' not found in columns: {gdf.columns.tolist()}\")\n",
    "            return None\n",
    "        if gdf.empty or gdf['geometry'].isnull().all():\n",
    "            logging.warning(f\"File {geojson_filename}: Empty or contains no valid geometries.\")\n",
    "            return None\n",
    "        # Remove rows with null geometry\n",
    "        gdf = gdf[gdf['geometry'].notna()]\n",
    "        if gdf.empty:\n",
    "            logging.warning(f\"File {geojson_filename}: No valid geometries after removing nulls.\")\n",
    "            return None\n",
    "\n",
    "        # --- CRS Handling ---\n",
    "        # Use gdf.crs.srs to get the string representation like 'EPSG:xxxx'\n",
    "        current_crs_str = gdf.crs.srs.upper() if gdf.crs else None\n",
    "        target_crs_str = crs.upper()\n",
    "\n",
    "        if current_crs_str and current_crs_str != target_crs_str:\n",
    "            logging.info(f\"Reprojecting {geojson_filename} from {current_crs_str} to {target_crs_str}...\")\n",
    "            gdf = gdf.to_crs(target_crs_str)\n",
    "        elif not current_crs_str:\n",
    "            # Attempt to guess common CRS if missing, otherwise error\n",
    "            try:\n",
    "                logging.warning(f\"File {geojson_filename} has no CRS defined. Assuming it's EPSG:4326 (WGS84) and reprojecting to {target_crs_str}. **VERIFY THIS ASSUMPTION!**\")\n",
    "                gdf.set_crs(f\"EPSG:4326\", inplace=True) # Common default, adjust if wrong!\n",
    "                gdf = gdf.to_crs(target_crs_str)\n",
    "            except Exception as crs_err:\n",
    "                logging.error(f\"Failed to set/reproject missing CRS for {geojson_filename}. Error: {crs_err}\")\n",
    "                return None\n",
    "        # Ensure target SRID is set explicitly in the GeoDataFrame's CRS attribute\n",
    "        gdf = gdf.set_crs(target_crs_str, allow_override=True)\n",
    "\n",
    "        # Keep only the name and geometry, standardize name column\n",
    "        gdf_processed = gdf[[name_property, 'geometry']].rename(columns={name_property: 'name'})\n",
    "\n",
    "        # Convert Polygons to MultiPolygons if needed by DB schema\n",
    "        gdf_processed['geometry'] = gdf_processed['geometry'].simplify(tolerance=0.0001, preserve_topology=True)\n",
    "        gdf_processed['geometry'] = gdf_processed.geometry.apply(lambda geom: MultiPolygon([geom]) if geom.geom_type == 'Polygon' else geom)\n",
    "\n",
    "        def get_vertex_count(geom):\n",
    "            if geom.is_empty:\n",
    "                return 0\n",
    "\n",
    "            if geom.geom_type == 'MultiPolygon':\n",
    "                count = 0\n",
    "                for poly in geom.geoms:\n",
    "                    count += len(poly.exterior.coords)\n",
    "                    count += sum(len(interior.coords) for interior in poly.interiors)\n",
    "                return count\n",
    "            else:\n",
    "                return 0\n",
    "        print(\"The number of point:\\n\", gdf_processed['geometry'].apply(get_vertex_count))\n",
    "\n",
    "\n",
    "        # Drop duplicates based on the name property within this file\n",
    "        gdf_processed = gdf_processed.drop_duplicates(subset=['name'])\n",
    "        logging.info(f\"Loaded {len(gdf_processed)} unique {expected_level} geometries from {geojson_filename}.\")\n",
    "        return gdf_processed\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading or processing file {geojson_filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fd3d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 22:46:41,632 - INFO - Attempting to load LGA geometries from: data/Project/Geojson files/LGA_2021_AUST_GDA94.geojson using name property: LGA_NAME21...\n",
      "/var/folders/zx/glnr12gn0pg2611688cv8bx00000gn/T/ipykernel_84126/2026990845.py:37: UserWarning: GeoSeries.notna() previously returned False for both missing (None) and empty geometries. Now, it only returns False for missing values. Since the calling GeoSeries contains empty geometries, the result has changed compared to previous versions of GeoPandas.\n",
      "Given a GeoSeries 's', you can use '~s.is_empty & s.notna()' to get back the old behaviour.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', 'GeoSeries.notna', UserWarning)\n",
      "  gdf = gdf[gdf['geometry'].notna()]\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/DataWarehousing/lib/python3.12/site-packages/shapely/constructive.py:863: RuntimeWarning: invalid value encountered in simplify_preserve_topology\n",
      "  return lib.simplify_preserve_topology(geometry, tolerance, **kwargs)\n",
      "2025-04-09 22:46:52,470 - INFO - Loaded 566 unique LGA geometries from LGA_2021_AUST_GDA94.geojson.\n",
      "2025-04-09 22:46:52,480 - INFO - Attempting to load SA4 geometries from: data/Project/Geojson files/SA4_2021_AUST_GDA2020.geojson using name property: SA4_NAME21...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of point:\n",
      " 0       897\n",
      "1      3510\n",
      "2      1179\n",
      "3      5076\n",
      "4      2966\n",
      "       ... \n",
      "561       0\n",
      "562    1507\n",
      "563       0\n",
      "564       0\n",
      "565       0\n",
      "Name: geometry, Length: 566, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/glnr12gn0pg2611688cv8bx00000gn/T/ipykernel_84126/2026990845.py:37: UserWarning: GeoSeries.notna() previously returned False for both missing (None) and empty geometries. Now, it only returns False for missing values. Since the calling GeoSeries contains empty geometries, the result has changed compared to previous versions of GeoPandas.\n",
      "Given a GeoSeries 's', you can use '~s.is_empty & s.notna()' to get back the old behaviour.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', 'GeoSeries.notna', UserWarning)\n",
      "  gdf = gdf[gdf['geometry'].notna()]\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/DataWarehousing/lib/python3.12/site-packages/shapely/constructive.py:863: RuntimeWarning: invalid value encountered in simplify_preserve_topology\n",
      "  return lib.simplify_preserve_topology(geometry, tolerance, **kwargs)\n",
      "2025-04-09 22:47:00,462 - INFO - Loaded 108 unique SA4 geometries from SA4_2021_AUST_GDA2020.geojson.\n",
      "2025-04-09 22:47:00,467 - INFO - Attempting to load State geometries from: data/Project/Geojson files/STE_2021_AUST_GDA2020.geojson using name property: STE_NAME21...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of point:\n",
      " 0      15236\n",
      "1       4144\n",
      "2      12005\n",
      "3       4139\n",
      "4       9555\n",
      "       ...  \n",
      "103        0\n",
      "104     3580\n",
      "105        0\n",
      "106        0\n",
      "107        0\n",
      "Name: geometry, Length: 108, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zx/glnr12gn0pg2611688cv8bx00000gn/T/ipykernel_84126/2026990845.py:37: UserWarning: GeoSeries.notna() previously returned False for both missing (None) and empty geometries. Now, it only returns False for missing values. Since the calling GeoSeries contains empty geometries, the result has changed compared to previous versions of GeoPandas.\n",
      "Given a GeoSeries 's', you can use '~s.is_empty & s.notna()' to get back the old behaviour.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', 'GeoSeries.notna', UserWarning)\n",
      "  gdf = gdf[gdf['geometry'].notna()]\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/DataWarehousing/lib/python3.12/site-packages/shapely/constructive.py:863: RuntimeWarning: invalid value encountered in simplify_preserve_topology\n",
      "  return lib.simplify_preserve_topology(geometry, tolerance, **kwargs)\n",
      "2025-04-09 22:47:06,866 - INFO - Loaded 10 unique State geometries from STE_2021_AUST_GDA2020.geojson.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of point:\n",
      " 0     55258\n",
      "1     33212\n",
      "2     74404\n",
      "3     38256\n",
      "4    122392\n",
      "5     94825\n",
      "6     41551\n",
      "7      1398\n",
      "8      3577\n",
      "9         0\n",
      "Name: geometry, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albury</td>\n",
       "      <td>MULTIPOLYGON (((146.86565 -36.07293, 146.86481...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Armidale Regional</td>\n",
       "      <td>MULTIPOLYGON (((151.32425 -30.26923, 151.32295...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ballina</td>\n",
       "      <td>MULTIPOLYGON (((153.57106 -28.87382, 153.57178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Balranald</td>\n",
       "      <td>MULTIPOLYGON (((143.00432 -33.78165, 143.03952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bathurst Regional</td>\n",
       "      <td>MULTIPOLYGON (((149.91212 -33.39582, 149.91144...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                           geometry\n",
       "0             Albury  MULTIPOLYGON (((146.86565 -36.07293, 146.86481...\n",
       "1  Armidale Regional  MULTIPOLYGON (((151.32425 -30.26923, 151.32295...\n",
       "2            Ballina  MULTIPOLYGON (((153.57106 -28.87382, 153.57178...\n",
       "3          Balranald  MULTIPOLYGON (((143.00432 -33.78165, 143.03952...\n",
       "4  Bathurst Regional  MULTIPOLYGON (((149.91212 -33.39582, 149.91144..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capital Region</td>\n",
       "      <td>MULTIPOLYGON (((150.05261 -37.26253, 150.05218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Central Coast</td>\n",
       "      <td>MULTIPOLYGON (((151.31497 -33.55578, 151.31509...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Central West</td>\n",
       "      <td>MULTIPOLYGON (((150.14236 -32.34153, 150.14372...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coffs Harbour - Grafton</td>\n",
       "      <td>MULTIPOLYGON (((153.07639 -30.42982, 153.07668...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Far West and Orana</td>\n",
       "      <td>MULTIPOLYGON (((148.67618 -29.50976, 148.67961...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                                           geometry\n",
       "0           Capital Region  MULTIPOLYGON (((150.05261 -37.26253, 150.05218...\n",
       "1            Central Coast  MULTIPOLYGON (((151.31497 -33.55578, 151.31509...\n",
       "2             Central West  MULTIPOLYGON (((150.14236 -32.34153, 150.14372...\n",
       "3  Coffs Harbour - Grafton  MULTIPOLYGON (((153.07639 -30.42982, 153.07668...\n",
       "4       Far West and Orana  MULTIPOLYGON (((148.67618 -29.50976, 148.67961..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New South Wales</td>\n",
       "      <td>MULTIPOLYGON (((159.0623 -31.50886, 159.06218 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>MULTIPOLYGON (((146.29286 -39.15778, 146.29341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Queensland</td>\n",
       "      <td>MULTIPOLYGON (((142.5314 -10.68301, 142.53072 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South Australia</td>\n",
       "      <td>MULTIPOLYGON (((140.66025 -38.06256, 140.66004...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Western Australia</td>\n",
       "      <td>MULTIPOLYGON (((117.86953 -35.19108, 117.86979...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                           geometry\n",
       "0    New South Wales  MULTIPOLYGON (((159.0623 -31.50886, 159.06218 ...\n",
       "1           Victoria  MULTIPOLYGON (((146.29286 -39.15778, 146.29341...\n",
       "2         Queensland  MULTIPOLYGON (((142.5314 -10.68301, 142.53072 ...\n",
       "3    South Australia  MULTIPOLYGON (((140.66025 -38.06256, 140.66004...\n",
       "4  Western Australia  MULTIPOLYGON (((117.86953 -35.19108, 117.86979..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lga_geom_gdf = load_geometries(\n",
    "    geojson_filename=\"LGA_2021_AUST_GDA94.geojson\",\n",
    "    name_property=\"LGA_NAME21\",\n",
    "    expected_level=\"LGA\",\n",
    "    crs=\"EPSG:4283\",  # GDA94\n",
    ")\n",
    "sa4_geom_gdf = load_geometries(\n",
    "    geojson_filename=\"SA4_2021_AUST_GDA2020.geojson\",\n",
    "    name_property=\"SA4_NAME21\",\n",
    "    expected_level=\"SA4\",\n",
    "    crs=\"EPSG:7844\",  # GDA2020\n",
    ")\n",
    "state_geom_gdf = load_geometries(\n",
    "    geojson_filename=\"STE_2021_AUST_GDA2020.geojson\",\n",
    "    name_property=\"STE_NAME21\",\n",
    "    expected_level=\"State\",\n",
    "    crs=\"EPSG:7844\",  # GDA2020\n",
    ")\n",
    "\n",
    "# Display heads of loaded geodataframes (optional check)\n",
    "if lga_geom_gdf is not None:\n",
    "    display(lga_geom_gdf.head())\n",
    "if sa4_geom_gdf is not None:\n",
    "    display(sa4_geom_gdf.head())\n",
    "if state_geom_gdf is not None:\n",
    "    display(state_geom_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7b8a39",
   "metadata": {},
   "source": [
    "#### Step 3: Transform and Generate Rows for Each Level\n",
    "\n",
    "This cell iterates through the different location levels (LGA, SA4, State, StateRemoteness), merges the attributes with the corresponding geometries loaded in the previous step, and prepares DataFrames for each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d2880a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 22:47:10,119 - INFO - Mapped full state names to abbreviations in state_geom_gdf.\n",
      "2025-04-09 22:47:10,126 - INFO - Determined location_level for each row.\n",
      "2025-04-09 22:47:10,129 - INFO - Merging LGA geometries (left join on 'lga_name_2021')...\n",
      "2025-04-09 22:47:10,135 - INFO - Merging SA4 geometries (left join on 'sa4_name_2021')...\n",
      "2025-04-09 22:47:10,137 - INFO - Merging State geometries (left join on 'state')...\n",
      "2025-04-09 22:47:10,141 - INFO - Finished merging geometries.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Location Level Distribution:\n",
      "location_level\n",
      "LGA      789\n",
      "State     13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Geometry Merge Summary:\n",
      " - LGA Geometries merged (not null): 763 / 802\n",
      " - SA4 Geometries merged (not null): 788 / 802\n",
      " - State Geometries merged (not null): 802 / 802\n",
      "\n",
      "Sample of final_gdf (before geometry conversion for CSV):\n",
      "  state                           sa4_name_2021      lga_name_2021  \\\n",
      "0   NSW                                Riverina        Wagga Wagga   \n",
      "1   NSW  Sydney - Baulkham Hills and Hawkesbury         Hawkesbury   \n",
      "2   Tas               Launceston and North East  Northern Midlands   \n",
      "3   NSW              New England and North West  Armidale Regional   \n",
      "4   Vic                                 Unknown            Unknown   \n",
      "\n",
      "            remoteness_area location_level  \\\n",
      "0  Inner Regional Australia            LGA   \n",
      "1  Inner Regional Australia            LGA   \n",
      "2  Inner Regional Australia            LGA   \n",
      "3  Outer Regional Australia            LGA   \n",
      "4                   Unknown          State   \n",
      "\n",
      "                                            lga_geom  \\\n",
      "0  MULTIPOLYGON (((147.44141 -34.96944, 147.44566...   \n",
      "1  MULTIPOLYGON (((150.61595 -33.61415, 150.61237...   \n",
      "2  MULTIPOLYGON (((147.09427 -41.51279, 147.10037...   \n",
      "3  MULTIPOLYGON (((151.32425 -30.26923, 151.32295...   \n",
      "4                                               None   \n",
      "\n",
      "                                            sa4_geom  \\\n",
      "0  MULTIPOLYGON (((147.28091 -35.49464, 147.28027...   \n",
      "1  MULTIPOLYGON (((151.15205 -33.52766, 151.1525 ...   \n",
      "2  MULTIPOLYGON (((146.6383 -39.47578, 146.63831 ...   \n",
      "3  MULTIPOLYGON (((151.02438 -28.79176, 151.02412...   \n",
      "4                                               None   \n",
      "\n",
      "                                          state_geom  \n",
      "0  MULTIPOLYGON (((159.0623 -31.50886, 159.06218 ...  \n",
      "1  MULTIPOLYGON (((159.0623 -31.50886, 159.06218 ...  \n",
      "2  MULTIPOLYGON (((144.60439 -41.01001, 144.60458...  \n",
      "3  MULTIPOLYGON (((159.0623 -31.50886, 159.06218 ...  \n",
      "4  MULTIPOLYGON (((146.29286 -39.15778, 146.29341...  \n"
     ]
    }
   ],
   "source": [
    "# --- 1. Preprocessing ---\n",
    "\n",
    "# Handle potential explicit 'Unknown' strings or NaNs consistently.\n",
    "# Replace specific string 'Unknown' with NaN for easier processing.\n",
    "unknown_value = 'Unknown'\n",
    "unique_locations_df = unique_locations_df.replace(unknown_value, np.nan)\n",
    "\n",
    "# Preprocess State GDF: Map full names to abbreviations\n",
    "if state_geom_gdf is not None and 'name' in state_geom_gdf.columns:\n",
    "    state_geom_gdf['name'] = state_geom_gdf['name'].replace(state_mapping)\n",
    "    logging.info(\"Mapped full state names to abbreviations in state_geom_gdf.\")\n",
    "else:\n",
    "    logging.warning(\"state_geom_gdf is None or 'name' column missing. Cannot map state names.\")\n",
    "\n",
    "# Ensure geometry GDFs have the correct CRS based on DB TARGET\n",
    "try:\n",
    "    if lga_geom_gdf is not None and lga_geom_gdf.crs.to_epsg() != LGA_TARGET_SRID:\n",
    "        logging.info(f\"Reprojecting LGA geometries to EPSG:{LGA_TARGET_SRID}...\")\n",
    "        lga_geom_gdf = lga_geom_gdf.to_crs(f\"EPSG:{LGA_TARGET_SRID}\")\n",
    "\n",
    "    if sa4_geom_gdf is not None and sa4_geom_gdf.crs.to_epsg() != SA4_STATE_TARGET_SRID:\n",
    "        logging.info(f\"Reprojecting SA4 geometries to EPSG:{SA4_STATE_TARGET_SRID}...\")\n",
    "        sa4_geom_gdf = sa4_geom_gdf.to_crs(f\"EPSG:{SA4_STATE_TARGET_SRID}\")\n",
    "\n",
    "    if state_geom_gdf is not None and state_geom_gdf.crs.to_epsg() != SA4_STATE_TARGET_SRID:\n",
    "        logging.info(f\"Reprojecting State geometries to EPSG:{SA4_STATE_TARGET_SRID}...\")\n",
    "        state_geom_gdf = state_geom_gdf.to_crs(f\"EPSG:{SA4_STATE_TARGET_SRID}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error setting/checking geometry CRS: {e}\")\n",
    "    # Decide how to proceed - stop or continue without correct CRS?\n",
    "\n",
    "# Rename geometry columns in source GDFs for clarity during merge\n",
    "if lga_geom_gdf is not None:\n",
    "    lga_geom_gdf = lga_geom_gdf.rename(columns={'geometry': 'lga_geom'})\n",
    "if sa4_geom_gdf is not None:\n",
    "    sa4_geom_gdf = sa4_geom_gdf.rename(columns={'geometry': 'sa4_geom'})\n",
    "if state_geom_gdf is not None:\n",
    "    state_geom_gdf = state_geom_gdf.rename(columns={'geometry': 'state_geom'})\n",
    "\n",
    "\n",
    "# --- 2. Determine Location Level ---\n",
    "\n",
    "def determine_location_level(row):\n",
    "    # Check from most specific to least specific\n",
    "    if pd.notna(row['lga_name_2021']):\n",
    "        return 'LGA'\n",
    "    elif pd.notna(row['sa4_name_2021']):\n",
    "        return 'SA4'\n",
    "    elif pd.notna(row['state']):\n",
    "        return 'State'\n",
    "    else:\n",
    "        return 'Unknown' # Should not happen if state is always present\n",
    "\n",
    "unique_locations_df['location_level'] = unique_locations_df.apply(determine_location_level, axis=1)\n",
    "logging.info(\"Determined location_level for each row.\")\n",
    "# Display value counts for verification\n",
    "print(\"\\nLocation Level Distribution:\")\n",
    "print(unique_locations_df['location_level'].value_counts())\n",
    "\n",
    "\n",
    "# --- 3. Merge Geometries ---\n",
    "\n",
    "# Start with the unique locations and levels\n",
    "final_gdf = unique_locations_df.copy()\n",
    "\n",
    "# Merge LGA geometries\n",
    "if lga_geom_gdf is not None:\n",
    "    logging.info(f\"Merging LGA geometries (left join on 'lga_name_2021')...\")\n",
    "    final_gdf = pd.merge(\n",
    "        final_gdf,\n",
    "        lga_geom_gdf[['name', 'lga_geom']],\n",
    "        how='left',\n",
    "        left_on='lga_name_2021',\n",
    "        right_on='name'\n",
    "    )\n",
    "    final_gdf = final_gdf.drop(columns=['name']) # Drop the merge key from gdf\n",
    "else:\n",
    "    logging.warning(\"LGA geometry GDF not available. Skipping LGA geometry merge.\")\n",
    "    final_gdf['lga_geom'] = None # Add empty column if GDF is missing\n",
    "\n",
    "# Merge SA4 geometries\n",
    "if sa4_geom_gdf is not None:\n",
    "    logging.info(f\"Merging SA4 geometries (left join on 'sa4_name_2021')...\")\n",
    "    final_gdf = pd.merge(\n",
    "        final_gdf,\n",
    "        sa4_geom_gdf[['name', 'sa4_geom']],\n",
    "        how='left',\n",
    "        left_on='sa4_name_2021',\n",
    "        right_on='name'\n",
    "    )\n",
    "    final_gdf = final_gdf.drop(columns=['name']) # Drop the merge key from gdf\n",
    "else:\n",
    "    logging.warning(\"SA4 geometry GDF not available. Skipping SA4 geometry merge.\")\n",
    "    final_gdf['sa4_geom'] = None\n",
    "\n",
    "# Merge State geometries\n",
    "if state_geom_gdf is not None:\n",
    "    logging.info(f\"Merging State geometries (left join on 'state')...\")\n",
    "    # Ensure state column types match (should both be string)\n",
    "    final_gdf['state'] = final_gdf['state'].astype(str)\n",
    "    state_geom_gdf['name'] = state_geom_gdf['name'].astype(str)\n",
    "\n",
    "    final_gdf = pd.merge(\n",
    "        final_gdf,\n",
    "        state_geom_gdf[['name', 'state_geom']],\n",
    "        how='left',\n",
    "        left_on='state',\n",
    "        right_on='name'\n",
    "    )\n",
    "    final_gdf = final_gdf.drop(columns=['name']) # Drop the merge key from gdf\n",
    "else:\n",
    "    logging.warning(\"State geometry GDF not available. Skipping State geometry merge.\")\n",
    "    final_gdf['state_geom'] = None\n",
    "\n",
    "logging.info(\"Finished merging geometries.\")\n",
    "\n",
    "# --- 4. Final Structure and Verification ---\n",
    "\n",
    "# Define the final expected column order matching dim_location (excluding location_key)\n",
    "all_columns = [\n",
    "    \"state\",\n",
    "    \"sa4_name_2021\",\n",
    "    \"lga_name_2021\",\n",
    "    \"remoteness_area\",\n",
    "    \"location_level\",\n",
    "    \"lga_geom\",\n",
    "    \"sa4_geom\",\n",
    "    \"state_geom\",\n",
    "]\n",
    "\n",
    "# Ensure all columns exist, adding missing ones as None if necessary\n",
    "for col in all_columns:\n",
    "    if col not in final_gdf.columns:\n",
    "        final_gdf[col] = None\n",
    "        logging.warning(f\"Column '{col}' was missing after merges, added as None.\")\n",
    "\n",
    "# Reorder columns and fill NaNs in non-geometry columns before export\n",
    "final_gdf = final_gdf[all_columns]\n",
    "final_gdf['state'] = final_gdf['state'].fillna('Unknown') # Reinstate Unknown if needed\n",
    "final_gdf['sa4_name_2021'] = final_gdf['sa4_name_2021'].fillna('Unknown')\n",
    "final_gdf['lga_name_2021'] = final_gdf['lga_name_2021'].fillna('Unknown')\n",
    "final_gdf['remoteness_area'] = final_gdf['remoteness_area'].fillna('Unknown')\n",
    "final_gdf['location_level'] = final_gdf['location_level'].fillna('Unknown')\n",
    "\n",
    "\n",
    "# Display info about merged geometries\n",
    "print(\"\\nGeometry Merge Summary:\")\n",
    "print(f\" - LGA Geometries merged (not null): {final_gdf['lga_geom'].notna().sum()} / {len(final_gdf)}\")\n",
    "print(f\" - SA4 Geometries merged (not null): {final_gdf['sa4_geom'].notna().sum()} / {len(final_gdf)}\")\n",
    "print(f\" - State Geometries merged (not null): {final_gdf['state_geom'].notna().sum()} / {len(final_gdf)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample of final_gdf (before geometry conversion for CSV):\")\n",
    "print(final_gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc360b45",
   "metadata": {},
   "source": [
    "#### Step 4: Write to CSV File\n",
    "\n",
    "Save the processed DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e41660be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:23:46,487 - INFO - Converting geometry columns to EWKT format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 23:24:28,726 - INFO - Geometry conversion complete.\n",
      "2025-04-09 23:24:28,762 - INFO - Exporting data to output/dim_location.csv...\n",
      "2025-04-09 23:24:47,784 - INFO - Successfully exported 802 rows to output/dim_location.csv.\n"
     ]
    }
   ],
   "source": [
    "# --- Convert Geometries to WKT with SRID Prefix ---\n",
    "def geometry_to_ewkt(geom, srid):\n",
    "    \"\"\"Converts a Shapely geometry object to EWKT format (SRID=xxxx;WKT).\"\"\"\n",
    "    if geom is None or pd.isna(geom):\n",
    "        return '' # Return empty string for NULL in COPY\n",
    "\n",
    "    # Ensure geometry is MultiPolygon if required by DB\n",
    "    # Note: This might be computationally intensive if not already MultiPolygons\n",
    "    if geom.geom_type == 'Polygon':\n",
    "        geom = MultiPolygon([geom])\n",
    "    elif geom.geom_type != 'MultiPolygon':\n",
    "        # Handle other types if necessary (e.g., GeometryCollection)\n",
    "        # For simplicity, returning empty string if not Polygon/MultiPolygon\n",
    "        # logging.warning(f\"Unexpected geometry type {geom.geom_type}, exporting as NULL.\")\n",
    "        return ''\n",
    "\n",
    "    # Use shapely.wkt.dumps for robust WKT conversion\n",
    "    wkt_string = wkt_dumps(geom, rounding_precision=10)  # Adjust precision as needed\n",
    "    return f\"SRID={srid};{wkt_string}\"\n",
    "\n",
    "logging.info(\"Converting geometry columns to EWKT format...\")\n",
    "final_gdf['lga_geom_wkt'] = final_gdf['lga_geom'].apply(lambda x: geometry_to_ewkt(x, LGA_TARGET_SRID))\n",
    "final_gdf['sa4_geom_wkt'] = final_gdf['sa4_geom'].apply(lambda x: geometry_to_ewkt(x, SA4_STATE_TARGET_SRID))\n",
    "final_gdf['state_geom_wkt'] = final_gdf['state_geom'].apply(lambda x: geometry_to_ewkt(x, SA4_STATE_TARGET_SRID))\n",
    "logging.info(\"Geometry conversion complete.\")\n",
    "\n",
    "# --- Select and Order Columns for CSV ---\n",
    "# Must match the order expected by COPY (excluding the auto-generated location_key)\n",
    "# Replace original geometry columns with their WKT versions\n",
    "csv_columns = [\n",
    "    \"state\",\n",
    "    \"sa4_name_2021\",\n",
    "    \"lga_name_2021\",\n",
    "    \"remoteness_area\",\n",
    "    \"location_level\",\n",
    "    \"lga_geom_wkt\", # Use WKT column\n",
    "    \"sa4_geom_wkt\", # Use WKT column\n",
    "    \"state_geom_wkt\", # Use WKT column\n",
    "]\n",
    "export_df_final = final_gdf[csv_columns]\n",
    "\n",
    "# --- Check VARCHAR Lengths (Optional but Recommended) ---\n",
    "varchar_limits = {\n",
    "    \"state\": 7,\n",
    "    \"sa4_name_2021\": 50,\n",
    "    \"lga_name_2021\": 50,\n",
    "    \"remoteness_area\": 50,\n",
    "    \"location_level\": 20\n",
    "}\n",
    "for col, limit in varchar_limits.items():\n",
    "    max_len = export_df_final[col].astype(str).str.len().max()\n",
    "    if max_len > limit:\n",
    "        logging.warning(f\"WARNING: Max length for column '{col}' ({max_len}) exceeds DB schema limit ({limit}). Data might be truncated on import.\")\n",
    "\n",
    "\n",
    "# --- Export to CSV ---\n",
    "logging.info(f\"Exporting data to {OUTPUT_CSV_FILE}...\")\n",
    "export_df_final.to_csv(\n",
    "    OUTPUT_CSV_FILE,\n",
    "    index=False,\n",
    ")\n",
    "logging.info(f\"Successfully exported {len(export_df_final)} rows to {OUTPUT_CSV_FILE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef72797",
   "metadata": {},
   "source": [
    "### dim_population_lga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "802280c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_cols = [str(y) for y in range(2001, 2024)]\n",
    "\n",
    "# Melt the raw population data\n",
    "pop_lga_melted = pd.melt(\n",
    "    pop_LGA_df,\n",
    "    id_vars=[\"lga_name_2021\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"population\",\n",
    ")\n",
    "\n",
    "# Basic cleaning and type conversion\n",
    "pop_lga_melted[\"year\"] = pop_lga_melted[\"year\"].astype(int)\n",
    "pop_lga_melted[\"population\"] = pd.to_numeric(\n",
    "    pop_lga_melted[\"population\"], errors=\"coerce\"\n",
    ")\n",
    "pop_lga_melted.dropna(subset=[\"population\"], inplace=True)\n",
    "pop_lga_melted[\"population\"] = pop_lga_melted[\"population\"].astype(int)\n",
    "\n",
    "# Merge melted population data with the location dimension data\n",
    "# This adds the 'location_key' based on matching 'lga_name_2021'.\n",
    "# 'how=\"inner\"' ensures only LGAs present in both DataFrames are kept.\n",
    "dim_location_keyed = final_gdf.copy()\n",
    "dim_location_keyed['location_key'] = range(1, len(dim_location_keyed) + 1)\n",
    "\n",
    "pop_lga_melted = pd.merge(\n",
    "    pop_lga_melted,\n",
    "    dim_location_keyed[['location_key', 'lga_name_2021']],\n",
    "    how='inner',\n",
    "    on='lga_name_2021'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "44be016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and reorder the final columns according to the target table structure\n",
    "dim_population_lga_df = pop_lga_melted[[\"location_key\", \"year\", \"population\"]]\n",
    "\n",
    "dim_population_lga_df.to_csv(\"output/dim_population_lga.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68062d5",
   "metadata": {},
   "source": [
    "### dim_population_remoteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b59655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign basic column names\n",
    "pop_remoteness_df.columns = [\"state_raw\", \"remoteness_raw\"] + year_cols\n",
    "\n",
    "# Forward fill the State column\n",
    "pop_remoteness_df[\"state_raw\"] = pop_remoteness_df[\"state_raw\"].ffill()\n",
    "\n",
    "# Melt\n",
    "pop_remote_melted = pd.melt(\n",
    "    pop_remoteness_df,\n",
    "    id_vars=[\"state_raw\", \"remoteness_raw\"],\n",
    "    value_vars=year_cols,\n",
    "    var_name=\"year\",\n",
    "    value_name=\"population\",\n",
    ")\n",
    "\n",
    "# Basic cleaning and type conversion\n",
    "pop_remote_melted[\"year\"] = pop_remote_melted[\"year\"].astype(int)\n",
    "pop_remote_melted[\"population\"] = pd.to_numeric(\n",
    "    pop_remote_melted[\"population\"], errors=\"coerce\"\n",
    ")\n",
    "pop_remote_melted.dropna(\n",
    "    subset=[\"population\", \"remoteness_raw\"], inplace=True\n",
    ")  # Drop rows with no population or remoteness name\n",
    "pop_remote_melted[\"population\"] = pop_remote_melted[\"population\"].astype(int)\n",
    "\n",
    "# Map state names\n",
    "pop_remote_melted[\"state\"] = (\n",
    "    pop_remote_melted[\"state_raw\"].map(state_mapping).fillna(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Basic Remoteness name cleaning (remove state suffix)\n",
    "pop_remote_melted[\"remoteness_area\"] = (\n",
    "    pop_remote_melted[\"remoteness_raw\"]\n",
    "    .astype(str)\n",
    "    .str.replace(r\"\\s*\\(.*\\)$\", \"\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Filter out unknown states and potentially totals/migratory rows if they cause issues\n",
    "pop_remote_melted = pop_remote_melted[pop_remote_melted[\"state\"] != \"Unknown\"]\n",
    "\n",
    "pop_remote_melted = pd.merge(\n",
    "    pop_remote_melted,\n",
    "    dim_location_keyed[[\"location_key\", \"state\", \"remoteness_area\"]],\n",
    "    how=\"inner\",\n",
    "    on=[\"state\", \"remoteness_area\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff2b16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final columns\n",
    "dim_population_remoteness_df = pop_remote_melted[[\"location_key\", \"year\", \"population\"]]\n",
    "dim_population_remoteness_df.to_csv(\"output/dim_population_remoteness.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d8170",
   "metadata": {},
   "source": [
    "### dim_dwelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9795d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dwellings_df.columns = [\"lga_name_2021\", \"dwelling_count\"]\n",
    "\n",
    "dwellings_df = pd.merge(\n",
    "    dwellings_df,\n",
    "    dim_location_keyed[[\"location_key\", \"lga_name_2021\"]],\n",
    "    how=\"inner\",\n",
    "    on=\"lga_name_2021\",\n",
    ")\n",
    "\n",
    "dwellings_csv = dwellings_df[[\"location_key\", \"dwelling_count\"]]\n",
    "\n",
    "dwellings_csv.to_csv(\"output/dim_dwelling.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24385c75-ab4a-4352-b64b-8fb9f28db20d",
   "metadata": {},
   "source": [
    "## Create Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb6ed3",
   "metadata": {},
   "source": [
    "### fact_fatality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 19:51:18,267 - INFO - Starting Fact Table Generation...\n",
      "2025-04-09 19:51:18,268 - INFO - Adding surrogate keys to dimension DataFrames...\n",
      "2025-04-09 19:51:18,318 - INFO - Surrogate keys added.\n",
      "2025-04-09 19:51:18,319 - INFO - Preparing fact_df for merging with dimension keys...\n",
      "2025-04-09 19:51:18,425 - INFO - fact_df prepared for merging.\n",
      "2025-04-09 19:51:18,425 - INFO - Merging dimension keys into fact data...\n",
      "2025-04-09 19:51:18,629 - INFO - Finished merging dimension keys.\n",
      "2025-04-09 19:51:18,631 - INFO - Checking for missing keys after merge:\n",
      "date_key          0\n",
      "time_key          0\n",
      "location_key      0\n",
      "crash_key         0\n",
      "road_user_key    34\n",
      "dtype: int64\n",
      "2025-04-09 19:51:18,632 - WARNING - Missing keys found! Rows with missing keys will be dropped.\n",
      "2025-04-09 19:51:18,654 - INFO - Creating fact_fatality DataFrame...\n",
      "2025-04-09 19:51:18,662 - INFO - Created fact_fatality_df with 56787 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>time_key</th>\n",
       "      <th>location_key</th>\n",
       "      <th>crash_key</th>\n",
       "      <th>road_user_key</th>\n",
       "      <th>fatality_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key  time_key  location_key  crash_key  road_user_key  fatality_count\n",
       "0         1         1             1          1              1               1\n",
       "1         2         2             2          2              2               1\n",
       "2         1         3             3          3              3               1\n",
       "3         2         4             4          4              4               1\n",
       "4         2         5             5          5              5               1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 19:51:18,717 - INFO - Successfully exported 56787 rows to fact_fatality.csv.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Starting Fact Table Generation...\")\n",
    "\n",
    "# --- Step 1: Add Surrogate Keys to Dimension DataFrames (Simulating DB Load) ---\n",
    "\n",
    "logging.info(\"Adding surrogate keys to dimension DataFrames...\")\n",
    "\n",
    "# Make copies to avoid modifying originals if needed elsewhere\n",
    "dim_date_keyed = dim_date_df.copy()\n",
    "dim_timeofday_keyed = dim_timeofday_df.copy()\n",
    "dim_roaduser_keyed = dim_roaduser_df.copy()\n",
    "dim_crash_keyed = dim_crash_df.copy()\n",
    "# Use the location DF BEFORE geometry was converted to WKT for merging\n",
    "dim_location_keyed = final_gdf.copy()\n",
    "\n",
    "# Add keys (starting from 1 like SERIAL/BIGSERIAL)\n",
    "dim_date_keyed['date_key'] = range(1, len(dim_date_keyed) + 1)\n",
    "dim_timeofday_keyed['time_key'] = range(1, len(dim_timeofday_keyed) + 1)\n",
    "dim_roaduser_keyed['road_user_key'] = range(1, len(dim_roaduser_keyed) + 1)\n",
    "dim_crash_keyed['crash_key'] = range(1, len(dim_crash_keyed) + 1)\n",
    "dim_location_keyed['location_key'] = range(1, len(dim_location_keyed) + 1)\n",
    "\n",
    "logging.info(\"Surrogate keys added.\")\n",
    "\n",
    "# Display sample of keyed dimensions (optional check)\n",
    "# print(\"Dim Date with Key (Head):\")\n",
    "# display(dim_date_keyed.head())\n",
    "# print(\"\\nDim Location with Key (Head):\")\n",
    "# display(dim_location_keyed.head())\n",
    "\n",
    "# --- Step 2: Prepare fact_df for Merging ---\n",
    "# Ensure data types and values used for merging match how dimensions were created\n",
    "\n",
    "logging.info(\"Preparing fact_df for merging with dimension keys...\")\n",
    "\n",
    "# Create a working copy\n",
    "fact_merged_df = fact_df.copy()\n",
    "\n",
    "# Time: Convert 'Time_victim' to datetime.time object for merging with dim_timeofday\n",
    "fact_merged_df['time_for_merge'] = pd.to_datetime(\n",
    "    fact_merged_df['Time_victim'], format='%H:%M:%S', errors='coerce'\n",
    ").dt.time\n",
    "\n",
    "# Hour_24: Re-calculate hour based on the cleaned time for merging consistency\n",
    "fact_merged_df['hour_24_for_merge'] = pd.to_datetime(\n",
    "    fact_merged_df['Time_victim'].astype(str), format='%H:%M:%S', errors='coerce'\n",
    ").dt.hour.astype('Int64') # Use Int64 to allow NA\n",
    "\n",
    "# Road User: Ensure 'Unknown' fill matches dim_roaduser creation\n",
    "fact_merged_df['Age Group'] = fact_merged_df['Age Group'].fillna(\"Unknown\")\n",
    "# Rename columns to match dim_roaduser_keyed\n",
    "fact_merged_df.rename(\n",
    "    columns={\n",
    "        \"Road User\": \"road_user_type\",\n",
    "        \"Gender\": \"gender\",\n",
    "        \"Age\": \"age\",\n",
    "        \"Age Group\": \"age_group\",\n",
    "        \"State_victim\": \"state_orig\", # Keep original names for location merge\n",
    "        \"SA4 Name 2021_victim\": \"sa4_name_2021_orig\",\n",
    "        \"National LGA Name 2021_victim\": \"lga_name_2021_orig\",\n",
    "        \"National Remoteness Areas_victim\": \"remoteness_area_orig\",\n",
    "        \"Time of day_victim\": \"time_of_day_category\", # Match dim_timeofday\n",
    "        \"Crash ID\": \"crash_id\", # Match dim_crash\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "# Handle potential NA's in Age before merge (convert to a type that handles NA, like float or Int64)\n",
    "fact_merged_df['age'] = fact_merged_df['age'].astype('Int64')\n",
    "\n",
    "# Location: Apply the same fillna logic used for creating dim_location\n",
    "location_cols_orig = [\"state_orig\", \"sa4_name_2021_orig\", \"lga_name_2021_orig\", \"remoteness_area_orig\"]\n",
    "for col in location_cols_orig:\n",
    "    fact_merged_df[col] = fact_merged_df[col].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# Crash: Ensure 'Unknown' fill matches dim_crash creation for relevant columns\n",
    "# (Only crash_id is needed for the merge key itself, but good practice if checking other attrs)\n",
    "# fact_merged_df['crash_type'] = fact_merged_df['Crash Type_victim'].fillna(\"Unknown\") # Example if needed\n",
    "\n",
    "logging.info(\"fact_df prepared for merging.\")\n",
    "\n",
    "# --- Step 3: Merge Dimension Keys into fact_merged_df ---\n",
    "\n",
    "logging.info(\"Merging dimension keys into fact data...\")\n",
    "\n",
    "# Merge Date Key\n",
    "fact_merged_df = pd.merge(\n",
    "    fact_merged_df,\n",
    "    dim_date_keyed[['year', 'month', 'christmas_period', 'easter_period', 'date_key']],\n",
    "    how='left',\n",
    "    left_on=['Year_victim', 'Month_victim', 'Christmas Period_victim', 'Easter Period_victim'],\n",
    "    right_on=['year', 'month', 'christmas_period', 'easter_period']\n",
    ")\n",
    "\n",
    "# Merge Time Key\n",
    "fact_merged_df = pd.merge(\n",
    "    fact_merged_df,\n",
    "    dim_timeofday_keyed[['time', 'time_of_day_category','hour_24', 'time_key']],\n",
    "    how='left',\n",
    "    left_on=['time_for_merge', 'time_of_day_category', 'hour_24_for_merge'],\n",
    "    right_on=['time', 'time_of_day_category', 'hour_24']\n",
    ")\n",
    "\n",
    "# Merge Road User Key\n",
    "fact_merged_df = pd.merge(\n",
    "    fact_merged_df,\n",
    "    dim_roaduser_keyed[['road_user_type', 'gender', 'age', 'age_group', 'road_user_key']],\n",
    "    how='left',\n",
    "    on=['road_user_type', 'gender', 'age', 'age_group'] # Use already renamed columns\n",
    ")\n",
    "\n",
    "# Merge Crash Key\n",
    "fact_merged_df = pd.merge(\n",
    "    fact_merged_df,\n",
    "    dim_crash_keyed[['crash_id', 'crash_key']],\n",
    "    how='left',\n",
    "    on='crash_id' # Use already renamed column\n",
    ")\n",
    "\n",
    "# Merge Location Key\n",
    "# Use the original location columns from fact_df (after fillna) and merge with dim_location_keyed\n",
    "fact_merged_df = pd.merge(\n",
    "    fact_merged_df,\n",
    "    dim_location_keyed[['state', 'sa4_name_2021', 'lga_name_2021', 'remoteness_area', 'location_level', 'location_key']],\n",
    "    how='left',\n",
    "    left_on=['state_orig', 'sa4_name_2021_orig', 'lga_name_2021_orig', 'remoteness_area_orig'],\n",
    "    right_on=['state', 'sa4_name_2021', 'lga_name_2021', 'remoteness_area']\n",
    ")\n",
    "\n",
    "logging.info(\"Finished merging dimension keys.\")\n",
    "\n",
    "# --- Step 4: Check for Missing Keys ---\n",
    "missing_keys = fact_merged_df[['date_key', 'time_key', 'location_key', 'crash_key', 'road_user_key']].isnull().sum()\n",
    "logging.info(f\"Checking for missing keys after merge:\\n{missing_keys}\")\n",
    "\n",
    "if missing_keys.sum() > 0:\n",
    "    logging.warning(\"Missing keys found! Rows with missing keys will be dropped.\")\n",
    "    # Optional: Inspect rows with missing keys\n",
    "    # display(fact_merged_df[fact_merged_df['location_key'].isnull()].head())\n",
    "    fact_merged_df.dropna(\n",
    "        subset=['date_key', 'time_key', 'location_key', 'crash_key', 'road_user_key'],\n",
    "        inplace=True\n",
    "    )\n",
    "    # Convert keys to integer after dropping NaNs\n",
    "    key_cols = ['date_key', 'time_key', 'location_key', 'crash_key', 'road_user_key']\n",
    "    fact_merged_df[key_cols] = fact_merged_df[key_cols].astype(int)\n",
    "\n",
    "\n",
    "# --- Step 5: Create fact_fatality_df ---\n",
    "logging.info(\"Creating fact_fatality DataFrame...\")\n",
    "\n",
    "# Add the measure\n",
    "fact_merged_df['fatality_count'] = 1\n",
    "\n",
    "# Select and order columns for the final fact table\n",
    "fact_fatality_df = fact_merged_df[[\n",
    "    'date_key',\n",
    "    'time_key',\n",
    "    'location_key',\n",
    "    'crash_key',\n",
    "    'road_user_key',\n",
    "    'fatality_count' # Measure\n",
    "]].copy()\n",
    "\n",
    "# Optional: Add primary key (though DB handles this)\n",
    "# fact_fatality_df.insert(0, 'fatality_key', range(1, len(fact_fatality_df) + 1))\n",
    "\n",
    "logging.info(f\"Created fact_fatality_df with {len(fact_fatality_df)} rows.\")\n",
    "display(fact_fatality_df.head())\n",
    "\n",
    "fact_fatality_df.to_csv(\"output/fact_fatality.csv\", index=False)\n",
    "logging.info(f\"Successfully exported {len(fact_fatality_df)} rows to fact_fatality.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataWarehousing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
